Optimizers tune the weight and biases to minimize loss value. This tweaking is what "trains" our model.

## ADAM (Adaptive Moment Estimation)

This optimizer basically uses gradient descent and moment to decide how to change the weight/bias.